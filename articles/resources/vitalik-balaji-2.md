**Vitalik:** I think the important thing is that crypto has been a symbolic talisman in a lot of those ways and also other ways for a long time. But there is a really big difference between a symbolic talisman and something that can actually do things for people that people directly need. To me, what's interesting is that Ethereum actually is crossing that chasm from being the first into being the second. And also, it's not just Ethereum. This is a part of a bigger ecosystem, and it goes beyond cryptocurrency, beyond blockchains, even beyond cryptography. One of the topics I've been talking about quite a bit recently is DAC and trying to create this decentralized defensive acceleration and trying to create this kind of bigger tent around basically...

## Vitalik on AI doom

**Balaji:** So on that topic, we're now about two and a half years after the ChatGPT debut. I want to argue about this maybe. So are you still an AI percentage doomer or would you call yourself... you had some non-zero percentage of it.

**Vitalik:** Yeah. My doom has gotten higher. The basic reason is that progress is happening faster than we expected. At the same time, global politics is worse than we expected. To me, the biggest unrealistic thing in classical doomerism is probably not so much in the problem but in the solution. One of the challenges I often see is... well, no, the naive one is the nice guy international treaties approach. Basically, the classical solution to this is, let's get the world to come together and agree to not do all of this stuff until we know how to do everything safely. At the same time, we're literally talking about a world where countries are invading each other and threatening to invade each other and blowing up tariffs to infinity and all kinds of different things.

**Balaji:** I want to argue on this because I think in my view, killer AI is already here and it's called drones. AI alignment cannot align an AI to everybody. It has to align it to its controller, and that could be one tribe or another tribe. Each tribe will have its own drones. AI alignment is not synonymous with AI safety and in fact is basically... the fact that killer AI equals drones, killer AI equals robots means I'm not actually concerned about image generators and text summarizers. Those aren't going to kill us. The robots would.

**Vitalik:** I'm not concerned about the image generators and summarizers either. I have the receipts for this where I publicly said that I don't expect the whole election deepfake thing to be a total nothingburger, and it has been. To me, the flip from danger being only sort of human to AI, to danger being potentially AI, started when AI crosses that roughly human-level kind of autonomy and generality threshold.

## Prompting is just higher order programming

**Balaji:** That autonomy is a big thing I want to poke on. To me, the biggest surprise and the biggest argument among my tech friends over the last two and a half years, especially around the time that ChatGPT came out, is will prompting endure? The fact that prompting has endured over the last two and a half years and it shows no signs of going away unless there's some enormous technological breakthrough means to me that prompting is just higher order programming, and it's just programming in English. You have programming in Python, you have programming in assembly. So if prompting doesn't go away, you don't actually have truly autonomous intelligence, you have amplified intelligence. It's not agentic intelligence.

**Vitalik:** Given that assumption, I agree. But there's this interesting chart that shows the time duration of tasks that AI can complete autonomously, and it has a doubling time of roughly every seven months. Historically, naively assuming that exponential curves will continue has been a much better AI prediction method than pretty much anything else. So if you naively follow the curve, then you basically get to AI completing tasks on the length of a human lifetime by about 2035.

**Balaji:** Maybe it's possible, but I think the transformer's coherence breaks down. There's certain kinds of things where training is starting to top out. You're hitting the available amounts of data out there, they need to generate them. I understand your argument, my counterargument is that I think genuine algorithmic breakthroughs, conceptual breakthroughs, are required. I can't say that's impossible, but I would say that right now prompting is much more of a constraint than people are giving it credit for.

The other thing is, and this is a counterargument to some of the stuff that I talk about, is I think he really underrates chaos, turbulence, and fundamentally mathematical systems or physical systems where predictability is genuinely constrained mathematically. You cannot predict beyond a certain window given finite precision arithmetic. You could, as a thought experiment, have an AI try to predict the motion of a fluid under turbulence. It can't do that without inventing math or maybe doing things we don't think are even possible. So that just shows that there are limits on what AI can just do as a purely computational thing, not an empirical thing. I don't know, the idea of one prompt and it just runs for the whole life...

**Vitalik:** I do think that there have been walls that have broken before. The big one that I think really updated me towards some more concern is the whole DeepSeek-R1 chain of thought thing. The reason why that updated me toward concern is because before that, we were in a regime where it felt like the thing that AI is fundamentally doing is just basically copying and kind of interpolating off of existing training data. If you just do that, then the logical outcome is that AI can do lots of things at roughly the level of the smartest humans, but then it tops off there and then you can't really go further.

**Balaji:** But the chain genuinely shows deliberation. It's like a smart person thinking about it.

**Vitalik:** That's right. Basically, the way it works is you start off with a model and then you ask it to solve problems where you can objectively determine if the answer is right or wrong. You have it run 10,000 chains of thought and give an answer. Then you filter for the ones where it got it right, and then you just train on that again and you repeat the loop. I think there, for the first time, we actually saw a training loop that could plausibly get us all the way to superhuman and there wasn't something that it's obviously constrained by.

**Balaji:** Interesting. It's because it's not just the probabilistic, it's also the multiple plans and then selecting from them. I don't know. Well, okay, here's a third piece, fourth piece. Here's how you would do this. This is the "how not to build the torment nexus" thing or whatever, but let's say one wanted to go about doing that. I think embodiment is obviously another big piece. I think that purely digital AI just can't do it. It's like a life form that only lives underwater. When it comes out onto air, no matter if it's a giant blue whale, it's going to run out of air.

**Vitalik:** I agree with this too, but Tesla Optimuses are pretty freaking impressive.

**Balaji:** Well, yeah. So, actually, the Unitrees and all these humanoids have gotten impressive.

**Vitalik:** Exactly.

**Balaji:** So the embodiment will get solved. But then it has to collect lots of sensor data and I think ultimately it has to have goals. Without goals, that's actually the very hardest thing. Humans, markets, and politics are domains where train and test doesn't work. If it worked, then you'd be able to just make tons of money in the market. You'd be able to win elections. These are inherently time-varying domains that AI can't crack right now because you don't actually have an approach to time-invariant domains.

**Vitalik:** Well, I do know that AI has been showing behavior that's more and more goal-like. It is a market incentive to try to train for that. It's a market incentive to try to create a thing where you can basically tell it, "Hey, make a web app for me," and then it just goes off for six hours and you don't have to think about it again, and then you get a web app at the end.

**Balaji:** Well, the question is though, there might be an MDL kind of argument here, minimum description length or like input complexity kind of thing where the number of bits... of course it's got the web or whatever, it's got it all cached there. So if you're looking up an existing solution, a small input is enough to pull that out. Like if you want a sudoku solver, you can just pull that out. But the more novel it is that you're trying to make something do, then the more input prompting you're going to need and it's not going to be like a one-shot that just gets it. That's my argument.

**Vitalik:** Right. But I guess in general, my prediction on that is for any particular category of thinking, at some point we are going to get to the point where AI has done it. We know it can do it because humans have done it, and humans are basically the result of just throwing some molecules in a soup and then doing about 10 to the 40 computation steps of evolution.

**Balaji:** So, I'm not making the argument that AI won't be able to think through bioinformatics or physics or something like that. I'm making the argument that it'll have to be prompted to do so because prompts are so high-dimensional that they're a direction vector in a very, very, very high-dimensional space. So, how is AI to search that direction and figure out what to do? For humans, it's basically like reproduction.

## Physical constraints on the Skynet scenario

**Balaji:** If you actually had the Skynet scenario where you had robots that could dig out data centers or whatever it was for the AI to live and they could actually mine the ore and set up the power generators, they get the full reproducible loop where they set up the factories to turn out more robots and have more AI to script it. Then you could actually have something where an AI could have a goal of reproducing or whatever, like a Terminator-like scenario. But, and that's not impossible physically. But the thing about that is probably prior to that, we probably have a lot of kill switches in these robots. A lot of the stuff was like, "Oh my god, it's going to learn how to bust out of the box and it's going to run on every computer like Skynet and it's going to explode out of containment." Do you still think that's possible? I can give you arguments as to why I don't think that's possible in that way.

**Vitalik:** I mean, I think AI getting out of containment like that part is possible. I can say what parts of the Doom pipeline I'm more skeptical of. Have you read the AI 2027 post?

**Balaji:** Yeah, but I think it's naive. Why do I think it's naive? For example, it assumes that it's like US versus China, and there's a lot of things that it gets wrong.

**Vitalik:** For me, the biggest technical thing that felt very implausible is that it assumed a world where, within their universe, they literally say this: "by January 2029, aging and cancer are both solved problems." Like, a cure for aging, a cure for cancer, they're both listed under emerging technologies. In this kind of world, it's absolutely impossible that we will not have solved pandemics. In 2030, it's assumed that the AI kills everyone with super-viruses. I think generally, those kinds of arguments are at their strongest when you try to step back and go into the abstract and you say, "Oh, well, see, you have no idea how Stockfish is going to beat you at chess, but you know that it is going to beat you." But the challenge with that metaphor is that if you provide a handicap, like if you take away a rook from Stockfish, then even today, grandmasters can a lot of the time beat top-ranking Stockfish AI. If you handicap the queen, then the human grandmasters just wipe the floor. There's only so much that a particular threshold of intelligence can actually give you.

**Balaji:** To that, actually, one of the most interesting things that I saw was with AlphaGo, you could play adversarial input to AlphaGo and you just beat AlphaGo. Meaning, for the people who don't know this, with AI image classification, because of the way it works, you can feed it an image that looks exactly like a dog but has a certain calculated layer of static on top of it that's invisible to the human eye, but that makes AI think it's a cat. That's called adversarial input. To my knowledge at least, there's been no way of defeating this. This is something which is just an intrinsic way of how deep learning works. I haven't seen a paper that does that.

This also applies, for example, to games. If you give an input to the AI that's just way outside its training data, you can win the game of Go by doing something that's totally crazy that AI has never seen before. It's actually funny, it's almost like that Hollywood movie intuition about AI is like you do something that the robot goes "beep" and it turns the wrong way because you're doing something very human that it can't expect.

To your other point about the cure cancer type stuff, this is, in my view, people... I have this concept in the *Network State* book of like God-State-Network, like what is the most powerful force in the world. These are people who have substituted AGI for God. What they do, I think, is they really, really underrate empiricism. I know something about biotech, you know something about biotech. You need to, whatever theory you have, and you might be able to actually do a lot in terms of taking all existing papers, indexing them, digesting them, coming up with theories on the basis... you can do a lot with that for sure, it's biomedical text mining. But then you have to test those theories. You have to actually go and have test tubes and you have to actually see if it works, and you just won't know if it works. You might have a theory, but you have to do practice.

**Vitalik:** I expect that sort of thing to be not an issue at all in some domains, but then a huge bottleneck in other domains.

**Balaji:** Embodiment and experiment are massive bottlenecks on AI. I mean, huge. They underrate the physical world dramatically. They underrate experiment dramatically.

**Vitalik:** It depends. For embodiment, if you look at the even the current bots, they already have a lot of advantages. Like if right now we had to bet on who would win a karate match between a human and a Tesla Optimus, what would you say?

**Balaji:** Oh, I mean, but like a gun... machines have been beating humans for a long time. I don't know, actually, like a karate match, I'm not sure how agile they are. I've seen the videos, I think they'll eventually get there.

**Vitalik:** Yeah, they'll get there.

**Balaji:** Well, I think realistically, they'll beat us on totally different domains. Like air is the big one. Okay. So, when I say embodiment, what I mean by that is you have to manufacture the robots. You have to have them be economically feasible. You have to transport them to the location. I'm not saying that these are unsolvable problems, but the friction of the physical world is radically... basically what we need to do for some of these scenarios to happen is you need to have as many humanoid robots as there are smartphones. Like a billion of them. That means you have to crack all kinds of business model type stuff. It'll eventually get there, but it's something where people are like, "Oh, cure AI by 20..." I just don't think their timelines are correct. I also think... what do you have to have? You'd have to have a robot that could do all the experiments that a human could do. They're at a bench, they're running those experiments, they have the reagents, you have to have the whole supply chain for them. This is just stuff which is friction that just doesn't move as fast as the internet. And I think they're underrating that.

**Vitalik:** I actually do think that there's enough of a risk that this stuff will happen very fast for us to worry. I think the probability of more humanoid robots than humans by 2040, I think it's very non-zero.

**Balaji:** I agree with that. It's funny, over the last two and a half years, I feel like AI has been both overrated and underrated. Here's why I say it: obviously, it's ridiculously important and in that sense underrated. I think the long term is right. But in terms of how people are actually using AI in practice, they're just being extremely lazy and using ChatGPT for essays. It's amazing how many people will just literally have ChatGPT write their tweets and I'm like, you don't think I can... It's like, you know, there's like a rhetorical question mark and a "it's not this, it's that" kind of thing.

There's certain things they do where it's like, actually, I want a Chrome plugin, maybe some of you guys can code this, that's like an AI that runs AI detection on every string on a site and then gives you a link. Because what'll happen is if you say to somebody that post is AI, sometimes they'll be like, "No, it's not," or whatever. And sometimes it actually isn't, right? It's kind of like an accusation of "you're dumb, that's AI," they get offended or whatever. So, a lot of secret AI. If you run "is AI," you get like an Etherscan link and it'll tell you why that was likely to be AI. Not Etherscan, but a link that's like that, a permalink. You paste that and you're like, "is AI, probability 73%," and you reply to that on X and it's like, "let me AI that for you or not AI that for you."

The way it's being used is like a... not all, but it's like an idiocratic version of it. The smarter you are, the better you use AI, and the dumber people are or the less taste they have, the worse they use AI. But that's... it just feels very different than any emergence path of AI that's been in science fiction or literature or anything like that.

**Vitalik:** The emergence path of AI has violated expectations in all kinds of different ways. It just repeatedly keeps on doing that. If you think of even the 1970s, they would not have expected AI to be able to solve arbitrarily complex arithmetic in 10 years but take 40 years to tell a cat from a dog. Even today, the line of what are the things AI can do and what are the things AI can't do is very difficult to describe. I think one of my theories is actually that the last tasks that humans will be able to do better than AI will be precisely the most illegible ones because those are the ones that are the hardest to train for.

**Balaji:** So what is your list? Because I'll give you my list.

**Vitalik:** It was interesting. I remember talking to someone. He asked me, "What is your benchmark for when you would say, 'yeah, this is AGI'?" And I said, "When an AI is able to independently start a profitable company."

**Balaji:** Interesting. I think I would argue we've already passed the threshold of AGI in some sense for some domains. Like it's better at math than many people. It's better at... it'll make mistakes, but people make mistakes at math. These two questions, "what can't AI do?" and "what can't China do?" I think are the most important questions for anybody who's doing a new startup. You build your business to assume AI or China will improve towards that and then you'll ride with that or at least be invulnerable to that rather than the opposite.

The thing that I'm seeing is AI right now, and maybe for the indefinite future, it doesn't do polish. It will actually... the whole thing about "AI takes a job," here's a reframe, which is AI lets you do any job at an okay level. You can be an okay artist, you can be an okay video editor, you can be an okay 3D designer or whatever. You can at least get your vision out onto paper. It's kind of getting to like a five level or something like that. It might be a cheesy, generic version, but you can do any job. In a sense, AI means a crude, improving form of digital autarky so long as you have access to the internet. China is physical because they're the most physically independent country in the world because they can build everything themselves. That's how I kind of think about it. But it's not getting really good at that area because the threshold for really good keeps moving forward as AI improves. To actually check the result, it might generate a math theorem, you actually have to be a research mathematician to even understand the symbols and check it.

**Vitalik:** The mental model I use is like if human level is normalized at some level like this, across a range of tasks, then basically AI kind of looks like this. It's better in some domains, worse in other domains. This curve just keeps shifting to the left in often very wobbly and unpredictable ways. From the perspective of the economy in 1800, by probably the year 2000, we basically had ASI from the perspective of probably over 80% of the economy. If you think of farming, what percentage of that was automated even by the year 2000? If you think of manufacturing... what I think is happening and the pattern that will continue for a while is basically this pattern where essentially both economic and social value continually refocus on the subset of tasks that AI hasn't just hyperinflated to zero.

At some point, we're going to enter a new regime where that set of human-dominant tasks actually shrinks to exactly zero. But we're not there yet. The question is what happens when they eventually get to the point where they can think a hundred times faster than you and at the same time they consume like 0.1 watts of energy and stuff.

**Balaji:** There's still comparative advantage, right? I mean, or rather, I guess that's a question. Does comparative advantage between AIs and humans exist? You can argue there isn't any with dolphins. Dolphins are just pets. There's nothing you can really delegate to them. They just can't figure it out. It's possible that it gets to that level, but I think at least until the prompting issue is solved, right now it really appears to me that artificial intelligence is just constrained by human intelligence. The better you are at prompting, the better the AI is. And conversely...

**Vitalik:** Even if you spell something wrong, it gives you a worse answer. I fully agree that that's the status quo today and probably at least for the next couple of years.

**Balaji:** Okay. Fine. All right. So, let's move to... let me talk about one more thing and let's go to questions. Startup societies, Zuzalu, and so on and so forth.

## Startup societies, network states, and Zuzalu

**Balaji:** After the *Network State* book came out, you actually... there's an old diagram I had, and then you kind of moved one more box on that diagram. Why don't you talk about that?

**Vitalik:** Basically, the box was... I mean, this was actually originally a diagram from your book, which is basically how many people are there and for how long a time do they come together. So it's like number of people, 200,000, a million, and then amount of time, one day, one week, a month, a year. You have Tinder over here and then you have eHarmony over here, you have universities over here, and then countries over here. With Zuzalu, the idea was that, okay, we've had lots of things that happen where people come together for a week, but there's this big discontinuous change that happens, I think, when you allow the duration of time to get longer.

The psychological shift that happens, and I'm sure a lot of you can probably relate to at this point, is that a week is a break from your life, two months is your life. If you stay in a place for two months, you actually fully readjust to that being the normal. It actually becomes a day-to-day rhythm as opposed to something where you've done a few days and there's a few days left and then you're gone.

On the other side is scale. There's what you can do with 10 people, which is a hacker house, but then there is what can you do when you get to Dunbar's number, which is 150. That's the level at which you start to just need to have multi-level structure in your society. Two months at 150 is this level where you actually start to get a lot of the complexities of doing something more meaningful at a larger scale, but at the same time, it's still small enough that it's practical to bring together and it's manageable. We've seen... Zuzalu was about 200 people for two months. I think it's a good range within which to experiment and try to figure out what are the actual kinds of things that we can do.

The general way that I see this is that this is a good scale at which you can dog-food things, at which you can actually start to go from having some new idea about how a civilization might work better to not just blabbing about it on the internet, which is what people did on internet forums back in 2009, but actually try it out and you actually get real-world information about where the successes and failures are.

**Balaji:** One of the things that's interesting is people in the community develop things for the community. The community was their beta testers for different types of... it's a full-stack incubator.

**Vitalik:** Exactly. There's a big difference between being a service provider and being a community. I think, for example, this is probably one of the things that the whole Estonia movement of 10 years ago got wrong. They had very advanced e-government for their time. You could vote online, open up bank accounts online, start companies online, all kinds of things, really advanced. But the difference between a product and a community is that a product is a hub-and-spoke model. It's N people who have a one-to-one relationship with some kind of center. A community is a model where you have N people that all have relationships with each other. In Estonia, I think the problem was that there was not enough commonality between the different people who were Estonia residents for them to actually really become a cohesive community.

**Balaji:** Exactly. It's like community is connectivity. Literally, of N people, you have N choose two possible relationships, or N squared if they're asymmetric, and then you divide how many relationships actually exist by the number of possible relationships. They just had a hub-and-spoke thing where everybody was connected to Estonia but not to each other.

**Vitalik:** One of the unique things that the community as incubator can do is that it actually gives you a community as a beta test user. There's a big difference between having 100 beta testers and having a community of 100 be your beta tester. The difference basically is it's the difference between 100 people who have no pre-existing relationship and need to interact with each other versus people who do have a lot of those needs every day. This is, to me, a very unique value proposition, that you can actually start a thing and then you can bootstrap and incubate it in a community and you can actually make a huge amount of progress together.

**Balaji:** That's right. I do think the startup society as a concept, what you're doing on the Zuzalu side, we're doing with Network School, we're helping bootstrap others. I think this is the third type of thing: internet company, internet currency, internet community. I've said that before, but I think it's the third type of thing that you can start.

**Vitalik:** One other thing that we started doing and that we've gotten into, and we really have been in a long time, is biotech.